{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7d301b0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5875, 22)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('parkinsons_updrs.csv')\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "ee88004b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject#</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>test_time</th>\n",
       "      <th>motor_UPDRS</th>\n",
       "      <th>total_UPDRS</th>\n",
       "      <th>Jitter(%)</th>\n",
       "      <th>Jitter(Abs)</th>\n",
       "      <th>Jitter:RAP</th>\n",
       "      <th>Jitter:PPQ5</th>\n",
       "      <th>...</th>\n",
       "      <th>Shimmer(dB)</th>\n",
       "      <th>Shimmer:APQ3</th>\n",
       "      <th>Shimmer:APQ5</th>\n",
       "      <th>Shimmer:APQ11</th>\n",
       "      <th>Shimmer:DDA</th>\n",
       "      <th>NHR</th>\n",
       "      <th>HNR</th>\n",
       "      <th>RPDE</th>\n",
       "      <th>DFA</th>\n",
       "      <th>PPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>5.6431</td>\n",
       "      <td>28.199</td>\n",
       "      <td>34.398</td>\n",
       "      <td>0.00662</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.00401</td>\n",
       "      <td>0.00317</td>\n",
       "      <td>...</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.01438</td>\n",
       "      <td>0.01309</td>\n",
       "      <td>0.01662</td>\n",
       "      <td>0.04314</td>\n",
       "      <td>0.014290</td>\n",
       "      <td>21.640</td>\n",
       "      <td>0.41888</td>\n",
       "      <td>0.54842</td>\n",
       "      <td>0.16006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>12.6660</td>\n",
       "      <td>28.447</td>\n",
       "      <td>34.894</td>\n",
       "      <td>0.00300</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.00132</td>\n",
       "      <td>0.00150</td>\n",
       "      <td>...</td>\n",
       "      <td>0.179</td>\n",
       "      <td>0.00994</td>\n",
       "      <td>0.01072</td>\n",
       "      <td>0.01689</td>\n",
       "      <td>0.02982</td>\n",
       "      <td>0.011112</td>\n",
       "      <td>27.183</td>\n",
       "      <td>0.43493</td>\n",
       "      <td>0.56477</td>\n",
       "      <td>0.10810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>19.6810</td>\n",
       "      <td>28.695</td>\n",
       "      <td>35.389</td>\n",
       "      <td>0.00481</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.00205</td>\n",
       "      <td>0.00208</td>\n",
       "      <td>...</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.00734</td>\n",
       "      <td>0.00844</td>\n",
       "      <td>0.01458</td>\n",
       "      <td>0.02202</td>\n",
       "      <td>0.020220</td>\n",
       "      <td>23.047</td>\n",
       "      <td>0.46222</td>\n",
       "      <td>0.54405</td>\n",
       "      <td>0.21014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>25.6470</td>\n",
       "      <td>28.905</td>\n",
       "      <td>35.810</td>\n",
       "      <td>0.00528</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.00191</td>\n",
       "      <td>0.00264</td>\n",
       "      <td>...</td>\n",
       "      <td>0.327</td>\n",
       "      <td>0.01106</td>\n",
       "      <td>0.01265</td>\n",
       "      <td>0.01963</td>\n",
       "      <td>0.03317</td>\n",
       "      <td>0.027837</td>\n",
       "      <td>24.445</td>\n",
       "      <td>0.48730</td>\n",
       "      <td>0.57794</td>\n",
       "      <td>0.33277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6420</td>\n",
       "      <td>29.187</td>\n",
       "      <td>36.375</td>\n",
       "      <td>0.00335</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.00093</td>\n",
       "      <td>0.00130</td>\n",
       "      <td>...</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.00679</td>\n",
       "      <td>0.00929</td>\n",
       "      <td>0.01819</td>\n",
       "      <td>0.02036</td>\n",
       "      <td>0.011625</td>\n",
       "      <td>26.126</td>\n",
       "      <td>0.47188</td>\n",
       "      <td>0.56122</td>\n",
       "      <td>0.19361</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject#  age  sex  test_time  motor_UPDRS  total_UPDRS  Jitter(%)  \\\n",
       "0         1   72    0     5.6431       28.199       34.398    0.00662   \n",
       "1         1   72    0    12.6660       28.447       34.894    0.00300   \n",
       "2         1   72    0    19.6810       28.695       35.389    0.00481   \n",
       "3         1   72    0    25.6470       28.905       35.810    0.00528   \n",
       "4         1   72    0    33.6420       29.187       36.375    0.00335   \n",
       "\n",
       "   Jitter(Abs)  Jitter:RAP  Jitter:PPQ5  ...  Shimmer(dB)  Shimmer:APQ3  \\\n",
       "0     0.000034     0.00401      0.00317  ...        0.230       0.01438   \n",
       "1     0.000017     0.00132      0.00150  ...        0.179       0.00994   \n",
       "2     0.000025     0.00205      0.00208  ...        0.181       0.00734   \n",
       "3     0.000027     0.00191      0.00264  ...        0.327       0.01106   \n",
       "4     0.000020     0.00093      0.00130  ...        0.176       0.00679   \n",
       "\n",
       "   Shimmer:APQ5  Shimmer:APQ11  Shimmer:DDA       NHR     HNR     RPDE  \\\n",
       "0       0.01309        0.01662      0.04314  0.014290  21.640  0.41888   \n",
       "1       0.01072        0.01689      0.02982  0.011112  27.183  0.43493   \n",
       "2       0.00844        0.01458      0.02202  0.020220  23.047  0.46222   \n",
       "3       0.01265        0.01963      0.03317  0.027837  24.445  0.48730   \n",
       "4       0.00929        0.01819      0.02036  0.011625  26.126  0.47188   \n",
       "\n",
       "       DFA      PPE  \n",
       "0  0.54842  0.16006  \n",
       "1  0.56477  0.10810  \n",
       "2  0.54405  0.21014  \n",
       "3  0.57794  0.33277  \n",
       "4  0.56122  0.19361  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9c5f0156",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n",
       "       35, 36, 37, 38, 39, 40, 41, 42], dtype=int64)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['age'].unique()\n",
    "df['subject#'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483fde63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "a4273ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns = ['motor_UPDRS', 'total_UPDRS','subject#','age','sex','test_time',\n",
    "                       'NHR','HNR','RPDE','DFA','PPE','Jitter(%)','Jitter(Abs)','Jitter:RAP',\n",
    "                       'Jitter:PPQ5','Jitter:DDP','Shimmer(dB)','Shimmer:APQ3','Shimmer:APQ5','Shimmer:APQ5','Shimmer:DDA'])\n",
    "y_motor = df['motor_UPDRS']\n",
    "y_total = df['total_UPDRS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "f76eb02b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Shimmer</th>\n",
       "      <th>Shimmer:APQ11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.02565</td>\n",
       "      <td>0.01662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02024</td>\n",
       "      <td>0.01689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.01675</td>\n",
       "      <td>0.01458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.02309</td>\n",
       "      <td>0.01963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.01703</td>\n",
       "      <td>0.01819</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Shimmer  Shimmer:APQ11\n",
       "0  0.02565        0.01662\n",
       "1  0.02024        0.01689\n",
       "2  0.01675        0.01458\n",
       "3  0.02309        0.01963\n",
       "4  0.01703        0.01819"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e5aa72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c02c994",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2311505d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "f287dc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train, y_test = train_test_split(X, y_motor, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "8a0a9d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f0d81c",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "a3064bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "linear_regression = LinearRegression()\n",
    "linear_regression.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred_test = linear_regression.predict(X_test_scaled)\n",
    "y_pred_train = linear_regression.predict(X_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "933bd760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 Score test: 0.022899580362018557\n",
      "R^2 Score train: 0.02366386189206926\n",
      "Mean Squared Error test: 62.36756338958154\n",
      "Mean Squared Error train: 65.0518413565818\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, r2_score, mean_squared_error\n",
    "\n",
    "\n",
    "#accuracy = accuracy_score(y_test, y_pred.round())\n",
    "#precision = precision_score(y_test, y_pred.round(), average='weighted')\n",
    "#recall = recall_score(y_test, y_pred.round(), average='weighted')\n",
    "#f1 = f1_score(y_test, y_pred.round(), average='weighted')\n",
    "r2_test = r2_score(y_test, y_pred_test)\n",
    "r2_train = r2_score(y_train, y_pred_train)\n",
    "\n",
    "mse_test = mean_squared_error(y_test, y_pred_test)\n",
    "mse_train = mean_squared_error(y_train, y_pred_train)\n",
    "\n",
    "#print(\"Accuracy:\", accuracy)\n",
    "#print(\"Precision:\", precision)\n",
    "#print(\"Recall:\", recall)\n",
    "#print(\"F1 Score:\", f1)\n",
    "print(\"R^2 Score test:\", r2_test)\n",
    "print(\"R^2 Score train:\", r2_train)\n",
    "print(\"Mean Squared Error test:\", mse_test)\n",
    "print(\"Mean Squared Error train:\", mse_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2d855c8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4112, 18)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239a7d33",
   "metadata": {},
   "source": [
    "# Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "54b5bd9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Sumit\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Sumit\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "Epoch 1/300\n",
      "WARNING:tensorflow:From C:\\Users\\Sumit\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 92.8121 - val_loss: 41.2190\n",
      "Epoch 2/300\n",
      "103/103 [==============================] - 0s 5ms/step - loss: 40.5898 - val_loss: 38.0324\n",
      "Epoch 3/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 31.7281 - val_loss: 30.4996\n",
      "Epoch 4/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 24.3806 - val_loss: 21.1372\n",
      "Epoch 5/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 20.7728 - val_loss: 23.1122\n",
      "Epoch 6/300\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 18.0629 - val_loss: 12.5997\n",
      "Epoch 7/300\n",
      "103/103 [==============================] - 0s 5ms/step - loss: 13.6578 - val_loss: 19.4545\n",
      "Epoch 8/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 13.3757 - val_loss: 25.9564\n",
      "Epoch 9/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 12.6230 - val_loss: 10.1516\n",
      "Epoch 10/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 10.9552 - val_loss: 9.2816\n",
      "Epoch 11/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 9.8630 - val_loss: 13.3698\n",
      "Epoch 12/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 9.8375 - val_loss: 11.5582\n",
      "Epoch 13/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 9.1584 - val_loss: 10.1791\n",
      "Epoch 14/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 8.6229 - val_loss: 8.0664\n",
      "Epoch 15/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 7.4427 - val_loss: 7.7614\n",
      "Epoch 16/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 7.7278 - val_loss: 6.1042\n",
      "Epoch 17/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 6.9865 - val_loss: 12.2589\n",
      "Epoch 18/300\n",
      "103/103 [==============================] - 0s 5ms/step - loss: 6.1665 - val_loss: 6.1534\n",
      "Epoch 19/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 7.8385 - val_loss: 8.4145\n",
      "Epoch 20/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 5.4822 - val_loss: 6.0101\n",
      "Epoch 21/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 5.5151 - val_loss: 6.3687\n",
      "Epoch 22/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 5.0154 - val_loss: 5.5208\n",
      "Epoch 23/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 5.0711 - val_loss: 6.8952\n",
      "Epoch 24/300\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 5.9006 - val_loss: 4.9411\n",
      "Epoch 25/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 4.3599 - val_loss: 6.8682\n",
      "Epoch 26/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 4.4985 - val_loss: 4.8437\n",
      "Epoch 27/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 4.6245 - val_loss: 8.4544\n",
      "Epoch 28/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 3.9218 - val_loss: 6.8459\n",
      "Epoch 29/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 4.4169 - val_loss: 10.5133\n",
      "Epoch 30/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 3.7584 - val_loss: 4.8039\n",
      "Epoch 31/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 3.5352 - val_loss: 3.8812\n",
      "Epoch 32/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 3.7563 - val_loss: 4.9071\n",
      "Epoch 33/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 3.6523 - val_loss: 4.2711\n",
      "Epoch 34/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 3.3359 - val_loss: 4.3761\n",
      "Epoch 35/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 2.9556 - val_loss: 6.2989\n",
      "Epoch 36/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 3.4294 - val_loss: 4.5315\n",
      "Epoch 37/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 2.9205 - val_loss: 5.0699\n",
      "Epoch 38/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 3.4263 - val_loss: 3.8554\n",
      "Epoch 39/300\n",
      "103/103 [==============================] - 0s 5ms/step - loss: 3.0621 - val_loss: 4.7108\n",
      "Epoch 40/300\n",
      "103/103 [==============================] - 0s 5ms/step - loss: 2.4985 - val_loss: 4.0532\n",
      "Epoch 41/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 3.2623 - val_loss: 3.5011\n",
      "Epoch 42/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 2.8134 - val_loss: 4.1150\n",
      "Epoch 43/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 2.4415 - val_loss: 5.5130\n",
      "Epoch 44/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 2.2054 - val_loss: 3.5123\n",
      "Epoch 45/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 2.2028 - val_loss: 3.1270\n",
      "Epoch 46/300\n",
      "103/103 [==============================] - 0s 5ms/step - loss: 2.3108 - val_loss: 8.9658\n",
      "Epoch 47/300\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 2.8124 - val_loss: 3.4387\n",
      "Epoch 48/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 2.2603 - val_loss: 9.1393\n",
      "Epoch 49/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 2.3027 - val_loss: 3.8081\n",
      "Epoch 50/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 2.6367 - val_loss: 4.4850\n",
      "Epoch 51/300\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 1.8498 - val_loss: 3.4751\n",
      "Epoch 52/300\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 2.1410 - val_loss: 3.0484\n",
      "Epoch 53/300\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 1.7777 - val_loss: 4.2129\n",
      "Epoch 54/300\n",
      "103/103 [==============================] - 0s 5ms/step - loss: 2.2864 - val_loss: 2.8220\n",
      "Epoch 55/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 1.8284 - val_loss: 3.3919\n",
      "Epoch 56/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 1.4701 - val_loss: 2.8252\n",
      "Epoch 57/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 1.6251 - val_loss: 2.7821\n",
      "Epoch 58/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 1.4760 - val_loss: 4.5277\n",
      "Epoch 59/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 1.8554 - val_loss: 3.1422\n",
      "Epoch 60/300\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 1.8863 - val_loss: 3.4492\n",
      "Epoch 61/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 1.5107 - val_loss: 2.5624\n",
      "Epoch 62/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 2.0562 - val_loss: 2.9944\n",
      "Epoch 63/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 1.2714 - val_loss: 3.0460\n",
      "Epoch 64/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 1.6047 - val_loss: 2.4805\n",
      "Epoch 65/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 1.3706 - val_loss: 2.4540\n",
      "Epoch 66/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 2.1454 - val_loss: 2.5960\n",
      "Epoch 67/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 1.5679 - val_loss: 5.0134\n",
      "Epoch 68/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 1.4951 - val_loss: 2.6543\n",
      "Epoch 69/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 1.5570 - val_loss: 3.0030\n",
      "Epoch 70/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 1.0908 - val_loss: 2.7449\n",
      "Epoch 71/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 1.3869 - val_loss: 8.3838\n",
      "Epoch 72/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 1.5362 - val_loss: 3.1873\n",
      "Epoch 73/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.9112 - val_loss: 2.6199\n",
      "Epoch 74/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 1.4993 - val_loss: 7.1349\n",
      "Epoch 75/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 1.5546 - val_loss: 2.6930\n",
      "Epoch 76/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 1.0944 - val_loss: 2.5531\n",
      "Epoch 77/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 1.1200 - val_loss: 2.5833\n",
      "Epoch 78/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 1.2263 - val_loss: 2.4680\n",
      "Epoch 79/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.9615 - val_loss: 2.4796\n",
      "Epoch 80/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 1.0034 - val_loss: 2.4419\n",
      "Epoch 81/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.9282 - val_loss: 2.9052\n",
      "Epoch 82/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.9973 - val_loss: 2.4902\n",
      "Epoch 83/300\n",
      "103/103 [==============================] - 0s 5ms/step - loss: 1.4188 - val_loss: 2.3166\n",
      "Epoch 84/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 1.3343 - val_loss: 2.4387\n",
      "Epoch 85/300\n",
      "103/103 [==============================] - 0s 5ms/step - loss: 0.9521 - val_loss: 2.1475\n",
      "Epoch 86/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 1.2844 - val_loss: 2.3042\n",
      "Epoch 87/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.7719 - val_loss: 3.0803\n",
      "Epoch 88/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.7307 - val_loss: 2.1113\n",
      "Epoch 89/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 1.5197 - val_loss: 2.3909\n",
      "Epoch 90/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.9504 - val_loss: 2.7172\n",
      "Epoch 91/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.8373 - val_loss: 2.5496\n",
      "Epoch 92/300\n",
      "103/103 [==============================] - 0s 5ms/step - loss: 0.6765 - val_loss: 2.7142\n",
      "Epoch 93/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.8612 - val_loss: 2.9900\n",
      "Epoch 94/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.9508 - val_loss: 3.9798\n",
      "Epoch 95/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.8650 - val_loss: 3.7088\n",
      "Epoch 96/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.7415 - val_loss: 2.2433\n",
      "Epoch 97/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.5812 - val_loss: 2.0510\n",
      "Epoch 98/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.7787 - val_loss: 2.4312\n",
      "Epoch 99/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.9814 - val_loss: 2.1345\n",
      "Epoch 100/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 1.2127 - val_loss: 2.7153\n",
      "Epoch 101/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.8797 - val_loss: 2.2490\n",
      "Epoch 102/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.7349 - val_loss: 2.2383\n",
      "Epoch 103/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.6139 - val_loss: 2.0576\n",
      "Epoch 104/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.8329 - val_loss: 2.9236\n",
      "Epoch 105/300\n",
      "103/103 [==============================] - 0s 5ms/step - loss: 0.6841 - val_loss: 1.9947\n",
      "Epoch 106/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.8066 - val_loss: 2.1198\n",
      "Epoch 107/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.5665 - val_loss: 2.0158\n",
      "Epoch 108/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.6398 - val_loss: 2.5664\n",
      "Epoch 109/300\n",
      "103/103 [==============================] - 0s 5ms/step - loss: 0.8231 - val_loss: 2.0993\n",
      "Epoch 110/300\n",
      "103/103 [==============================] - 0s 5ms/step - loss: 0.6091 - val_loss: 2.2363\n",
      "Epoch 111/300\n",
      "103/103 [==============================] - 0s 5ms/step - loss: 0.7449 - val_loss: 2.1274\n",
      "Epoch 112/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.5521 - val_loss: 2.1679\n",
      "Epoch 113/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.6517 - val_loss: 2.4510\n",
      "Epoch 114/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.5298 - val_loss: 1.9983\n",
      "Epoch 115/300\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.5113 - val_loss: 1.9658\n",
      "Epoch 116/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.6643 - val_loss: 3.9670\n",
      "Epoch 117/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.9772 - val_loss: 2.1341\n",
      "Epoch 118/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.8126 - val_loss: 2.1128\n",
      "Epoch 119/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.5232 - val_loss: 2.2187\n",
      "Epoch 120/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.4336 - val_loss: 2.0445\n",
      "Epoch 121/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.5132 - val_loss: 2.0049\n",
      "Epoch 122/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.5412 - val_loss: 3.8160\n",
      "Epoch 123/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.5446 - val_loss: 2.0099\n",
      "Epoch 124/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.5678 - val_loss: 1.9492\n",
      "Epoch 125/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.5498 - val_loss: 1.9499\n",
      "Epoch 126/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.4298 - val_loss: 2.9132\n",
      "Epoch 127/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.7046 - val_loss: 2.4157\n",
      "Epoch 128/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.4079 - val_loss: 1.9419\n",
      "Epoch 129/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.6095 - val_loss: 2.4335\n",
      "Epoch 130/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.4973 - val_loss: 2.2573\n",
      "Epoch 131/300\n",
      "103/103 [==============================] - 0s 5ms/step - loss: 0.4870 - val_loss: 2.1875\n",
      "Epoch 132/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.4837 - val_loss: 2.1233\n",
      "Epoch 133/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.7697 - val_loss: 1.9676\n",
      "Epoch 134/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.3677 - val_loss: 1.8813\n",
      "Epoch 135/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.4850 - val_loss: 1.8517\n",
      "Epoch 136/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.6005 - val_loss: 1.8648\n",
      "Epoch 137/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.3770 - val_loss: 1.9157\n",
      "Epoch 138/300\n",
      "103/103 [==============================] - 0s 5ms/step - loss: 0.3637 - val_loss: 1.8729\n",
      "Epoch 139/300\n",
      "103/103 [==============================] - 0s 5ms/step - loss: 0.3568 - val_loss: 1.8852\n",
      "Epoch 140/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.4004 - val_loss: 1.8259\n",
      "Epoch 141/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.5099 - val_loss: 1.8755\n",
      "Epoch 142/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.2756 - val_loss: 1.7739\n",
      "Epoch 143/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.2815 - val_loss: 1.8095\n",
      "Epoch 144/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.8864 - val_loss: 1.8914\n",
      "Epoch 145/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.5338 - val_loss: 1.9597\n",
      "Epoch 146/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.3998 - val_loss: 1.7602\n",
      "Epoch 147/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.3419 - val_loss: 2.1043\n",
      "Epoch 148/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.4069 - val_loss: 1.8273\n",
      "Epoch 149/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.3507 - val_loss: 1.8028\n",
      "Epoch 150/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.4446 - val_loss: 1.7941\n",
      "Epoch 151/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.3845 - val_loss: 1.9061\n",
      "Epoch 152/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.3235 - val_loss: 2.0710\n",
      "Epoch 153/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.2920 - val_loss: 1.8993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 154/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.4111 - val_loss: 2.0798\n",
      "Epoch 155/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.4181 - val_loss: 1.7599\n",
      "Epoch 156/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.7642 - val_loss: 1.8717\n",
      "Epoch 157/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.3587 - val_loss: 1.8122\n",
      "Epoch 158/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.2384 - val_loss: 1.8045\n",
      "Epoch 159/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.2294 - val_loss: 1.8793\n",
      "Epoch 160/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.2902 - val_loss: 1.7693\n",
      "Epoch 161/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.2614 - val_loss: 2.1296\n",
      "Epoch 162/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.3251 - val_loss: 2.4912\n",
      "Epoch 163/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.2944 - val_loss: 1.7931\n",
      "Epoch 164/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.3118 - val_loss: 1.8204\n",
      "Epoch 165/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.2578 - val_loss: 1.8790\n",
      "Epoch 166/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.2625 - val_loss: 1.8074\n",
      "Epoch 167/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.5238 - val_loss: 2.6456\n",
      "Epoch 168/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.3201 - val_loss: 1.8832\n",
      "Epoch 169/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.2289 - val_loss: 2.0709\n",
      "Epoch 170/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.3127 - val_loss: 1.7505\n",
      "Epoch 171/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.2875 - val_loss: 2.5230\n",
      "Epoch 172/300\n",
      "103/103 [==============================] - 0s 5ms/step - loss: 0.2668 - val_loss: 1.7952\n",
      "Epoch 173/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.1963 - val_loss: 1.7476\n",
      "Epoch 174/300\n",
      "103/103 [==============================] - 0s 5ms/step - loss: 0.3146 - val_loss: 1.7635\n",
      "Epoch 175/300\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.2724 - val_loss: 1.9235\n",
      "Epoch 176/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.3026 - val_loss: 1.7889\n",
      "Epoch 177/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.1922 - val_loss: 1.7289\n",
      "Epoch 178/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.3164 - val_loss: 1.7306\n",
      "Epoch 179/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.2727 - val_loss: 1.8444\n",
      "Epoch 180/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.3898 - val_loss: 2.4621\n",
      "Epoch 181/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.3515 - val_loss: 1.7290\n",
      "Epoch 182/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.2349 - val_loss: 1.6604\n",
      "Epoch 183/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.3884 - val_loss: 1.7921\n",
      "Epoch 184/300\n",
      "103/103 [==============================] - 0s 5ms/step - loss: 0.2382 - val_loss: 1.7207\n",
      "Epoch 185/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.2056 - val_loss: 1.6750\n",
      "Epoch 186/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.3821 - val_loss: 1.8508\n",
      "Epoch 187/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.3465 - val_loss: 1.7044\n",
      "Epoch 188/300\n",
      "103/103 [==============================] - 0s 5ms/step - loss: 0.2257 - val_loss: 1.7504\n",
      "Epoch 189/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.2350 - val_loss: 1.6731\n",
      "Epoch 190/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.2915 - val_loss: 1.7203\n",
      "Epoch 191/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.1863 - val_loss: 1.7014\n",
      "Epoch 192/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.2127 - val_loss: 1.6982\n",
      "Epoch 193/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.2611 - val_loss: 1.6834\n",
      "Epoch 194/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.1663 - val_loss: 2.0958\n",
      "Epoch 195/300\n",
      "103/103 [==============================] - 0s 5ms/step - loss: 0.2491 - val_loss: 1.7123\n",
      "Epoch 196/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.1628 - val_loss: 1.6424\n",
      "Epoch 197/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.4952 - val_loss: 1.6849\n",
      "Epoch 198/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.4961 - val_loss: 1.7950\n",
      "Epoch 199/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.2226 - val_loss: 1.6787\n",
      "Epoch 200/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.2629 - val_loss: 2.0712\n",
      "Epoch 201/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.2264 - val_loss: 1.6530\n",
      "Epoch 202/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.1950 - val_loss: 1.6725\n",
      "Epoch 203/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.2196 - val_loss: 1.6797\n",
      "Epoch 204/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.5191 - val_loss: 2.1826\n",
      "Epoch 205/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.1971 - val_loss: 1.7079\n",
      "Epoch 206/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.2718 - val_loss: 1.6871\n",
      "Epoch 207/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.2726 - val_loss: 2.2765\n",
      "Epoch 208/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.1751 - val_loss: 1.7683\n",
      "Epoch 209/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.3468 - val_loss: 1.7059\n",
      "Epoch 210/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.1698 - val_loss: 1.6327\n",
      "Epoch 211/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.2189 - val_loss: 1.7673\n",
      "Epoch 212/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.1995 - val_loss: 1.6554\n",
      "Epoch 213/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.2890 - val_loss: 1.6445\n",
      "Epoch 214/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.2151 - val_loss: 1.6965\n",
      "Epoch 215/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.1310 - val_loss: 1.9621\n",
      "Epoch 216/300\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.1932 - val_loss: 1.6890\n",
      "Epoch 217/300\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.1626 - val_loss: 1.7775\n",
      "Epoch 218/300\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.1264 - val_loss: 1.6613\n",
      "Epoch 219/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.1540 - val_loss: 1.8011\n",
      "Epoch 220/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.1298 - val_loss: 1.6523\n",
      "Epoch 221/300\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.1278 - val_loss: 2.2399\n",
      "Epoch 222/300\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.1964 - val_loss: 1.6718\n",
      "Epoch 223/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.2345 - val_loss: 1.6337\n",
      "Epoch 224/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.2023 - val_loss: 1.7580\n",
      "Epoch 225/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.1453 - val_loss: 1.7531\n",
      "Epoch 226/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.1146 - val_loss: 1.7706\n",
      "Epoch 227/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.1985 - val_loss: 1.7303\n",
      "Epoch 228/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.1437 - val_loss: 1.6531\n",
      "Epoch 229/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.1994 - val_loss: 1.6397\n",
      "Epoch 230/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.3692 - val_loss: 1.6965\n",
      "Epoch 231/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.1838 - val_loss: 1.7441\n",
      "Epoch 232/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.1545 - val_loss: 1.6857\n",
      "Epoch 233/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.1616 - val_loss: 1.6214\n",
      "Epoch 234/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.1377 - val_loss: 1.5816\n",
      "Epoch 235/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.2879 - val_loss: 2.8391\n",
      "Epoch 236/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.3251 - val_loss: 1.9379\n",
      "Epoch 237/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.1394 - val_loss: 1.6632\n",
      "Epoch 238/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.1189 - val_loss: 1.7425\n",
      "Epoch 239/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.1570 - val_loss: 1.6522\n",
      "Epoch 240/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.1008 - val_loss: 1.6806\n",
      "Epoch 241/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.0961 - val_loss: 1.6561\n",
      "Epoch 242/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.2890 - val_loss: 1.7184\n",
      "Epoch 243/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.1154 - val_loss: 1.7922\n",
      "Epoch 244/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.1348 - val_loss: 1.8033\n",
      "Epoch 245/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.1188 - val_loss: 1.6289\n",
      "Epoch 246/300\n",
      "103/103 [==============================] - 0s 5ms/step - loss: 0.1195 - val_loss: 1.6967\n",
      "Epoch 247/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.1069 - val_loss: 1.6139\n",
      "Epoch 248/300\n",
      "103/103 [==============================] - 0s 5ms/step - loss: 0.1328 - val_loss: 1.5778\n",
      "Epoch 249/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.1342 - val_loss: 1.7090\n",
      "Epoch 250/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.1456 - val_loss: 1.7261\n",
      "Epoch 251/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.1740 - val_loss: 1.6467\n",
      "Epoch 252/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.1376 - val_loss: 1.5840\n",
      "Epoch 253/300\n",
      "103/103 [==============================] - 0s 5ms/step - loss: 0.1495 - val_loss: 1.6386\n",
      "Epoch 254/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.1071 - val_loss: 1.7898\n",
      "Epoch 255/300\n",
      "103/103 [==============================] - 0s 5ms/step - loss: 0.2436 - val_loss: 1.7908\n",
      "Epoch 256/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.1597 - val_loss: 1.6458\n",
      "Epoch 257/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.3371 - val_loss: 1.6325\n",
      "Epoch 258/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.1419 - val_loss: 1.5944\n",
      "Epoch 259/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.1952 - val_loss: 1.6278\n",
      "Epoch 260/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.1044 - val_loss: 1.8192\n",
      "Epoch 261/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.0987 - val_loss: 1.5614\n",
      "Epoch 262/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.1396 - val_loss: 1.5907\n",
      "Epoch 263/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.2366 - val_loss: 1.5909\n",
      "Epoch 264/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.0999 - val_loss: 1.5901\n",
      "Epoch 265/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.0933 - val_loss: 1.7538\n",
      "Epoch 266/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.0916 - val_loss: 1.6394\n",
      "Epoch 267/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.1169 - val_loss: 2.3068\n",
      "Epoch 268/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.1326 - val_loss: 1.6072\n",
      "Epoch 269/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.0780 - val_loss: 1.7362\n",
      "Epoch 270/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.0824 - val_loss: 1.6696\n",
      "Epoch 271/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.2971 - val_loss: 1.6149\n",
      "Epoch 272/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.1173 - val_loss: 1.6507\n",
      "Epoch 273/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.1520 - val_loss: 1.6137\n",
      "Epoch 274/300\n",
      "103/103 [==============================] - 0s 5ms/step - loss: 0.1170 - val_loss: 1.5688\n",
      "Epoch 275/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.1015 - val_loss: 1.7696\n",
      "Epoch 276/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.1194 - val_loss: 1.6462\n",
      "Epoch 277/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.2340 - val_loss: 1.6099\n",
      "Epoch 278/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.1066 - val_loss: 1.6686\n",
      "Epoch 279/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.1129 - val_loss: 1.5918\n",
      "Epoch 280/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.1192 - val_loss: 1.5968\n",
      "Epoch 281/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.0678 - val_loss: 1.6020\n",
      "Epoch 282/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.1040 - val_loss: 1.6142\n",
      "Epoch 283/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.1708 - val_loss: 1.6342\n",
      "Epoch 284/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.0912 - val_loss: 1.5976\n",
      "Epoch 285/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.1107 - val_loss: 1.5580\n",
      "Epoch 286/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.0757 - val_loss: 1.6684\n",
      "Epoch 287/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.0912 - val_loss: 1.5898\n",
      "Epoch 288/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.0998 - val_loss: 1.6002\n",
      "Epoch 289/300\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.0981 - val_loss: 1.6565\n",
      "Epoch 290/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.0844 - val_loss: 1.6870\n",
      "Epoch 291/300\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.1031 - val_loss: 1.6394\n",
      "Epoch 292/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.1623 - val_loss: 1.5767\n",
      "Epoch 293/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.1485 - val_loss: 1.5925\n",
      "Epoch 294/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.1244 - val_loss: 1.6074\n",
      "Epoch 295/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.0702 - val_loss: 1.5995\n",
      "Epoch 296/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.1227 - val_loss: 1.7507\n",
      "Epoch 297/300\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.0833 - val_loss: 1.9634\n",
      "Epoch 298/300\n",
      "103/103 [==============================] - 0s 5ms/step - loss: 0.0745 - val_loss: 1.6113\n",
      "Epoch 299/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.2728 - val_loss: 1.6521\n",
      "Epoch 300/300\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.1287 - val_loss: 1.6539\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 1.3630\n",
      "Test Loss: 1.3630002737045288\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "input_size = 20\n",
    "hidden1_size = 500\n",
    "hidden2_size = 250\n",
    "hidden3_size = 100\n",
    "hidden4_size = 50\n",
    "output_size = 1\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(hidden1_size, input_shape=(input_size,), activation='relu'),\n",
    "    tf.keras.layers.Dense(hidden2_size, activation='relu'),\n",
    "    tf.keras.layers.Dense(hidden3_size, activation='relu'),\n",
    "    tf.keras.layers.Dense(hidden4_size, activation='relu'),\n",
    "    tf.keras.layers.Dense(output_size)\n",
    "])\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, \n",
    "              loss='mean_squared_error')\n",
    "\n",
    "history = model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, validation_split=0.2)\n",
    "\n",
    "loss = model.evaluate(X_test_scaled, y_test)\n",
    "print(\"Test Loss:\", loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e0c312",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8c857a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'C': 10.0, 'epsilon': 1.0, 'kernel': 'rbf'}\n",
      "Mean Squared Error: 41.69402712230681\n",
      "R-squared Score: 0.3467878303482219\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "svr_model = SVR()\n",
    "param_grid = {\n",
    "    'kernel': ['linear', 'rbf'],  \n",
    "    'C': [0.1, 1.0, 10.0],         \n",
    "    'epsilon': [0.01, 0.1, 1.0]    \n",
    "}\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(estimator = svr_model, param_grid = param_grid, scoring='neg_mean_squared_error', cv=5)\n",
    "\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "y_pred = best_model.predict(X_test_scaled)\n",
    "\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared Score:\", r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f99e2d",
   "metadata": {},
   "source": [
    "# Grid-Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0eeaba74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model Parameters: {'regressor': LinearRegression(), 'scaler': StandardScaler()}\n",
      "Mean Squared Error: 52.54249315762369\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'scaler': [None, StandardScaler()],\n",
    "    'regressor': [LinearRegression()]\n",
    "}\n",
    "\n",
    "\n",
    "scorer = make_scorer(mean_squared_error, greater_is_better=False)\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(estimator = pipeline, param_grid = param_grid, scoring = scorer, cv=5)\n",
    "\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Best Model Parameters:\", grid_search.best_params_)\n",
    "print(\"Mean Squared Error:\", mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6b5c74",
   "metadata": {},
   "source": [
    "# Decision Tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6d1844da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'criterion': 'squared_error', 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "Test MSE: 0.5390031411798072\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "param_grid = {\n",
    "    'criterion': ['squared_error', 'absolute_error', 'poisson'],\n",
    "    'max_depth': [None, 5, 10, 15],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "dt_regressor = DecisionTreeRegressor()\n",
    "\n",
    "grid_search = GridSearchCV(estimator = dt_regressor, param_grid = param_grid, cv = 5, scoring = 'neg_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "y_pred = best_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Test MSE:\", mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "47f50fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 44.64167583446954\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "\n",
    "rf_regressor = RandomForestRegressor(n_estimators=100, max_depth=10, min_samples_split=3, min_samples_leaf=2, max_features='sqrt', random_state=42)\n",
    "rf_regressor.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = rf_regressor.predict(X_test)\n",
    "\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "16425e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 54.14155861292096\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "\n",
    "dt_regressor = DecisionTreeRegressor(max_depth=10, min_samples_split=8, min_samples_leaf=10, random_state=30)\n",
    "dt_regressor.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = dt_regressor.predict(X_test)\n",
    "\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c824f162",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d461a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079fe43b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
